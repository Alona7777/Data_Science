{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640d7d77-5454-4885-b08c-733676ef47fa",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "640d7d77-5454-4885-b08c-733676ef47fa",
        "outputId": "423192bf-be4b-4fd7-a4b8-57b5d2611b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('думи', 2), ('мої', 2), ('лихо', 3), ('мені', 1), ('з', 1), ('вами', 1), ('нащо', 1), ('стали', 1), ('на', 4), ('папері', 1)]\n",
            "(77, 1000)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_1 (SimpleRNN)    (None, 128)               144512    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1000)              129000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 273512 (1.04 MB)\n",
            "Trainable params: 273512 (1.04 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 2s 18ms/step - loss: 6.9086 - accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 6.8587 - accuracy: 0.0811\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 6.8133 - accuracy: 0.3108\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 6.7631 - accuracy: 0.5405\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 6.7048 - accuracy: 0.6622\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 6.6307 - accuracy: 0.7297\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 6.5300 - accuracy: 0.7162\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 6.3860 - accuracy: 0.6622\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 6.1727 - accuracy: 0.4595\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 5.8586 - accuracy: 0.3243\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 5.3982 - accuracy: 0.2297\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 4.7980 - accuracy: 0.1486\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 4.1886 - accuracy: 0.1486\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 3.7984 - accuracy: 0.1486\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 3.6510 - accuracy: 0.1081\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 3.6097 - accuracy: 0.1081\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 3.5873 - accuracy: 0.1081\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 3.5708 - accuracy: 0.1081\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 3.5528 - accuracy: 0.1081\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 3.5296 - accuracy: 0.1081\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 3.5115 - accuracy: 0.1486\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.4814 - accuracy: 0.1622\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 3.4553 - accuracy: 0.1622\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 3.4179 - accuracy: 0.1622\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.3832 - accuracy: 0.1622\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.3381 - accuracy: 0.1622\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 3.2910 - accuracy: 0.1757\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.2405 - accuracy: 0.2027\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.1848 - accuracy: 0.2027\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.1250 - accuracy: 0.2568\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.0658 - accuracy: 0.2703\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 3.0043 - accuracy: 0.3514\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 2.9361 - accuracy: 0.3649\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 2.8654 - accuracy: 0.3649\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.7932 - accuracy: 0.3784\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 2.7154 - accuracy: 0.3919\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 2.6341 - accuracy: 0.3919\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.5542 - accuracy: 0.4054\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.4671 - accuracy: 0.4189\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.3816 - accuracy: 0.4459\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.2963 - accuracy: 0.4730\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 2.2113 - accuracy: 0.5270\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.1246 - accuracy: 0.5541\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.0414 - accuracy: 0.5811\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 1.9596 - accuracy: 0.5811\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.8815 - accuracy: 0.6081\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 1.8011 - accuracy: 0.6351\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7254 - accuracy: 0.6622\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 1.6525 - accuracy: 0.6892\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 1.5816 - accuracy: 0.7432\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Думи мої думи вас лихо на світ на сміх породило сміх сльози чом не затопили не винесли в винесли не питали не винесли\n"
          ]
        }
      ],
      "source": [
        "#ЗАВДАННЯ 1. ПРОГНОЗ СЛІВ\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Текст, який ви хочете використовувати\n",
        "texts = \"\"\"\n",
        "Думи мої, думи мої,\n",
        "Лихо мені з вами!\n",
        "Нащо стали на папері\n",
        "Сумними рядами?..\n",
        "Чом вас вітер не розвіяв\n",
        "В степу, як пилину?\n",
        "Чом вас лихо не приспало,\n",
        "Як свою дитину?..\n",
        "\n",
        "Бо вас лихо на світ на сміх породило,\n",
        "Поливали сльози... чом не затопили,\n",
        "Не винесли в море, не розмили в полі?.\n",
        "Не питали б люде, що в мене болить,\n",
        "Не питали б, за що проклинаю долю,\n",
        "Чого нуджу світом? \"Нічого робить\", —\n",
        "Не сказали б на сміх...\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "texts = texts.replace('\\ufeff', '')  # Прибираємо перший невидимий символ\n",
        "\n",
        "maxWordsCount = 1000\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&;()*+,-./:<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
        "                      lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts([texts])\n",
        "\n",
        "dist = list(tokenizer.word_counts.items())\n",
        "print(dist[:10])\n",
        "\n",
        "data = tokenizer.texts_to_sequences([texts])\n",
        "res = to_categorical(data[0], num_classes=maxWordsCount)\n",
        "print(res.shape)\n",
        "\n",
        "inp_words = 3\n",
        "n = res.shape[0] - inp_words\n",
        "\n",
        "X = np.array([res[i:i + inp_words, :] for i in range(n)])\n",
        "Y = res[inp_words:]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input((inp_words, maxWordsCount)))\n",
        "model.add(SimpleRNN(128, activation='tanh'))\n",
        "model.add(Dense(maxWordsCount, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
        "\n",
        "\n",
        "def buildPhrase(texts, str_len=20):\n",
        "    res = texts\n",
        "    data = tokenizer.texts_to_sequences([texts])[0]\n",
        "    for i in range(str_len):\n",
        "        x = to_categorical(data[i: i + inp_words], num_classes=maxWordsCount)  # преобразуем в One-Hot-encoding\n",
        "        inp = x.reshape(1, inp_words, maxWordsCount)\n",
        "\n",
        "        pred = model.predict(inp)\n",
        "        indx = pred.argmax(axis=1)[0]\n",
        "        data.append(indx)\n",
        "\n",
        "        res += \" \" + tokenizer.index_word[indx]  # Дописуємо рядок\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "res = buildPhrase(\"Думи мої думи\")\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56b5a1e-f46e-46e2-a313-9b6835b314f7",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56b5a1e-f46e-46e2-a313-9b6835b314f7",
        "outputId": "59d390ce-4d7a-46fe-d103-80938acb7d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('думи', 2), ('мої', 2), ('лихо', 3), ('мені', 1), ('з', 1), ('вами', 1), ('нащо', 1), ('стали', 1), ('на', 4), ('папері', 1)]\n",
            "(77,)\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 3, 256)            256000    \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1000)              129000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 434280 (1.66 MB)\n",
            "Trainable params: 434280 (1.66 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 2s 13ms/step - loss: 6.9082 - accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 6.8397 - accuracy: 0.3243\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 6.7743 - accuracy: 0.6351\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 6.6949 - accuracy: 0.7162\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 6.5902 - accuracy: 0.6486\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 6.4403 - accuracy: 0.5541\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 6.2255 - accuracy: 0.3784\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 5.8766 - accuracy: 0.2297\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 5.3730 - accuracy: 0.1216\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 4.7237 - accuracy: 0.1081\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 4.1052 - accuracy: 0.1081\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 3.8089 - accuracy: 0.1081\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 3.7012 - accuracy: 0.1081\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 3.6129 - accuracy: 0.1081\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 3.5385 - accuracy: 0.1081\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 3.4658 - accuracy: 0.1216\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 3.3950 - accuracy: 0.1892\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 3.3140 - accuracy: 0.2162\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 3.2207 - accuracy: 0.2973\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 3.1142 - accuracy: 0.3108\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 2.9993 - accuracy: 0.2838\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 2.8805 - accuracy: 0.3243\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 2.7676 - accuracy: 0.3378\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 2.6625 - accuracy: 0.3378\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 2.5496 - accuracy: 0.3784\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 2.4313 - accuracy: 0.3919\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 2.3102 - accuracy: 0.4324\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.1929 - accuracy: 0.5270\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.0785 - accuracy: 0.5405\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 1.9681 - accuracy: 0.5946\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.8619 - accuracy: 0.6351\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7529 - accuracy: 0.6622\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.6525 - accuracy: 0.6892\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.5548 - accuracy: 0.7297\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.4594 - accuracy: 0.7432\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.3724 - accuracy: 0.7703\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.2889 - accuracy: 0.7838\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.2135 - accuracy: 0.8108\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.1391 - accuracy: 0.8378\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.0679 - accuracy: 0.8649\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.0042 - accuracy: 0.8919\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.9449 - accuracy: 0.8919\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.8868 - accuracy: 0.8919\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.8348 - accuracy: 0.9054\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.7850 - accuracy: 0.9189\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.7393 - accuracy: 0.9324\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.6942 - accuracy: 0.9459\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.6545 - accuracy: 0.9459\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.6189 - accuracy: 0.9595\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.5836 - accuracy: 0.9595\n",
            "1/1 [==============================] - 0s 303ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Думи мої думи мої лихо мені з вами нащо стали на папері сумними рядами чом вас вітер не розвіяв в море як розмили\n"
          ]
        }
      ],
      "source": [
        "#Завдання 2. Прогнозування слів з Embedding\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Input, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Текст, який ви хочете використовувати\n",
        "texts = \"\"\"\n",
        "Думи мої, думи мої,\n",
        "Лихо мені з вами!\n",
        "Нащо стали на папері\n",
        "Сумними рядами?..\n",
        "Чом вас вітер не розвіяв\n",
        "В степу, як пилину?\n",
        "Чом вас лихо не приспало,\n",
        "Як свою дитину?..\n",
        "\n",
        "Бо вас лихо на світ на сміх породило,\n",
        "Поливали сльози... чом не затопили,\n",
        "Не винесли в море, не розмили в полі?.\n",
        "Не питали б люде, що в мене болить,\n",
        "Не питали б, за що проклинаю долю,\n",
        "Чого нуджу світом? \"Нічого робить\", —\n",
        "Не сказали б на сміх...\n",
        "\n",
        "\"\"\"\n",
        "texts = texts.replace('\\ufeff', '')  # Прибираємо перший невид. символ\n",
        "\n",
        "maxWordsCount = 1000\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
        "                      lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts([texts])\n",
        "\n",
        "dist = list(tokenizer.word_counts.items())\n",
        "print(dist[:10])\n",
        "\n",
        "data = tokenizer.texts_to_sequences([texts])\n",
        "\n",
        "res = np.array( data[0] )\n",
        "print(res.shape)\n",
        "\n",
        "inp_words = 3\n",
        "n = res.shape[0] - inp_words\n",
        "\n",
        "X = np.array([res[i:i + inp_words] for i in range(n)])\n",
        "Y = to_categorical(res[inp_words:], num_classes=maxWordsCount)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(maxWordsCount, 256, input_length = inp_words))\n",
        "model.add(SimpleRNN(128, activation='tanh'))\n",
        "model.add(Dense(maxWordsCount, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
        "\n",
        "\n",
        "def buildPhrase(texts, str_len=20):\n",
        "    res = texts\n",
        "    data = tokenizer.texts_to_sequences([texts])[0]\n",
        "    for i in range(str_len):\n",
        "        x = data[i: i + inp_words]\n",
        "        inp = np.expand_dims(x, axis=0)\n",
        "\n",
        "        pred = model.predict(inp)\n",
        "        indx = pred.argmax(axis=1)[0]\n",
        "        data.append(indx)\n",
        "\n",
        "        res += \" \" + tokenizer.index_word[indx]  # дописываем строку\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "res = buildPhrase(\"Думи мої думи\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c02daf9-ddd6-438d-a07f-b593d7d5c3a1",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c02daf9-ddd6-438d-a07f-b593d7d5c3a1",
        "outputId": "7c279cbe-60b1-47cc-da93-c0bedf9231ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('думи', 2), ('мої', 2), ('лихо', 3), ('мені', 1), ('з', 1), ('вами', 1), ('нащо', 1), ('стали', 1), ('на', 4), ('папері', 1)]\n",
            "(77,)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 3, 128)            128000    \n",
            "                                                                 \n",
            " simple_rnn_4 (SimpleRNN)    (None, 3, 128)            32896     \n",
            "                                                                 \n",
            " simple_rnn_5 (SimpleRNN)    (None, 64)                12352     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1000)              65000     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 238248 (930.66 KB)\n",
            "Trainable params: 238248 (930.66 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 2s 10ms/step - loss: 6.9034 - accuracy: 0.0135\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 6.8394 - accuracy: 0.2432\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 6.7752 - accuracy: 0.4324\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 6.6881 - accuracy: 0.5270\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 6.5664 - accuracy: 0.4595\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 6.3821 - accuracy: 0.3108\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 6.1173 - accuracy: 0.2162\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 5.7784 - accuracy: 0.1351\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 5.3896 - accuracy: 0.1216\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 5.0351 - accuracy: 0.1081\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 4.7349 - accuracy: 0.1081\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 4.4886 - accuracy: 0.1081\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 4.2917 - accuracy: 0.1081\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 4.1214 - accuracy: 0.1081\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 3.9899 - accuracy: 0.1081\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 3.8735 - accuracy: 0.1081\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 3.7744 - accuracy: 0.1081\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 3.6814 - accuracy: 0.1081\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 3.6021 - accuracy: 0.1081\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 3.5360 - accuracy: 0.1081\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 3.4720 - accuracy: 0.1486\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 3.4167 - accuracy: 0.1757\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 3.3580 - accuracy: 0.2027\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 3.2995 - accuracy: 0.2568\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 3.2470 - accuracy: 0.2703\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 3.1940 - accuracy: 0.2703\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 3.1426 - accuracy: 0.2973\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 3.0903 - accuracy: 0.3108\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 3.0363 - accuracy: 0.3108\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.9844 - accuracy: 0.3108\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.9306 - accuracy: 0.3108\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.8779 - accuracy: 0.3514\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 2.8226 - accuracy: 0.3649\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.7677 - accuracy: 0.3919\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.7153 - accuracy: 0.3919\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.6614 - accuracy: 0.3919\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.6060 - accuracy: 0.4189\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.5503 - accuracy: 0.4324\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 2.4949 - accuracy: 0.4459\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 2.4392 - accuracy: 0.4730\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 2.3863 - accuracy: 0.5000\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 2.3300 - accuracy: 0.5270\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.2734 - accuracy: 0.5541\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.2213 - accuracy: 0.5541\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.1686 - accuracy: 0.5541\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 2.1148 - accuracy: 0.5676\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 2.0611 - accuracy: 0.5676\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 2.0094 - accuracy: 0.6081\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 1.9565 - accuracy: 0.6351\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 1.9053 - accuracy: 0.6757\n",
            "1/1 [==============================] - 0s 257ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Думи мої думи мені лихо мені б на б в сміх в не сльози не степу в свою чом чом вас не не\n"
          ]
        }
      ],
      "source": [
        "#Завдання 3. Побудова моделі RNN\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Input, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Текст, який ви хочете використовувати\n",
        "texts = \"\"\"\n",
        "Думи мої, думи мої,\n",
        "Лихо мені з вами!\n",
        "Нащо стали на папері\n",
        "Сумними рядами?..\n",
        "Чом вас вітер не розвіяв\n",
        "В степу, як пилину?\n",
        "Чом вас лихо не приспало,\n",
        "Як свою дитину?..\n",
        "\n",
        "Бо вас лихо на світ на сміх породило,\n",
        "Поливали сльози... чом не затопили,\n",
        "Не винесли в море, не розмили в полі?.\n",
        "Не питали б люде, що в мене болить,\n",
        "Не питали б, за що проклинаю долю,\n",
        "Чого нуджу світом? \"Нічого робить\", —\n",
        "Не сказали б на сміх...\n",
        "\n",
        "\"\"\"\n",
        "texts = texts.replace('\\ufeff', '')  # Прибираємо перший невид. символ\n",
        "\n",
        "maxWordsCount = 1000\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»',\n",
        "                      lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts([texts])\n",
        "\n",
        "dist = list(tokenizer.word_counts.items())\n",
        "print(dist[:10])\n",
        "\n",
        "data = tokenizer.texts_to_sequences([texts])\n",
        "\n",
        "res = np.array( data[0] )\n",
        "print(res.shape)\n",
        "\n",
        "inp_words = 3\n",
        "n = res.shape[0] - inp_words\n",
        "\n",
        "X = np.array([res[i:i + inp_words] for i in range(n)])\n",
        "Y = to_categorical(res[inp_words:], num_classes=maxWordsCount)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(maxWordsCount, 128, input_length = inp_words))\n",
        "model.add(SimpleRNN(128, activation='tanh', return_sequences=True))\n",
        "model.add(SimpleRNN(64, activation='tanh'))\n",
        "model.add(Dense(maxWordsCount, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
        "\n",
        "\n",
        "def buildPhrase(texts, str_len=20):\n",
        "    res = texts\n",
        "    data = tokenizer.texts_to_sequences([texts])[0]\n",
        "    for i in range(str_len):\n",
        "        x = data[i: i + inp_words]\n",
        "        inp = np.expand_dims(x, axis=0)\n",
        "\n",
        "        pred = model.predict(inp)\n",
        "        indx = pred.argmax(axis=1)[0]\n",
        "        data.append(indx)\n",
        "\n",
        "        res += \" \" + tokenizer.index_word[indx]  # дописываем строку\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "res = buildPhrase(\"Думи мої думи\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f753b282-b7d9-4d37-b6cd-68076c1301ea",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f753b282-b7d9-4d37-b6cd-68076c1301ea",
        "outputId": "15fb74c0-7d97-4a37-b7ba-91f9d7887615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 3 6\n",
            "[('це', 2), ('текст', 2), ('який', 2), ('містить', 2), ('позитивну', 1), ('інформацію', 2), ('ще', 1), ('один', 1), ('приклад', 1), ('позитивного', 1)]\n",
            "Це текст, який містить позитивну інформацію.\n",
            "[[ 0  0  0  0  2  3  4  5  7  6]\n",
            " [ 0  0  0  0  0  8  9 10 11 12]\n",
            " [ 0  0  0  1 13 14 15 16 17 18]\n",
            " [ 0  0  0  0  2  3  4  5 19  6]\n",
            " [ 0  0  0  0  0 20 21  1 22 23]\n",
            " [ 0  0  0  0  0 24 25 26 27  1]]\n",
            "[('тексти', 1), ('це', 2), ('текст', 3), ('який', 4), ('містить', 5), ('інформацію', 6), ('позитивну', 7), ('ще', 8), ('один', 9), ('приклад', 10), ('позитивного', 11), ('тексту', 12), ('можуть', 13), ('містити', 14), ('різні', 15), ('емоції', 16), ('включаючи', 17), ('позитивні', 18), ('негативну', 19), ('не', 20), ('всі', 21), ('будуть', 22), ('позитивними', 23), ('можна', 24), ('зустріти', 25), ('і', 26), ('негативні', 27)]\n",
            "(6, 10) (6, 2)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 10, 128)           128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 10, 128)           131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 309122 (1.18 MB)\n",
            "Trainable params: 309122 (1.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6934 - accuracy: 0.3333\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6928 - accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6923 - accuracy: 0.6667\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6917 - accuracy: 0.8333\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6911 - accuracy: 0.8333\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6905 - accuracy: 0.8333\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6899 - accuracy: 0.8333\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6894 - accuracy: 0.8333\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6888 - accuracy: 0.8333\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6882 - accuracy: 0.8333\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6875 - accuracy: 0.8333\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6869 - accuracy: 0.8333\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6863 - accuracy: 0.8333\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6857 - accuracy: 0.8333\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6850 - accuracy: 0.8333\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6843 - accuracy: 0.8333\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6837 - accuracy: 0.8333\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6830 - accuracy: 0.8333\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6822 - accuracy: 0.8333\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6815 - accuracy: 0.8333\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6807 - accuracy: 0.8333\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6800 - accuracy: 0.8333\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6791 - accuracy: 0.8333\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6783 - accuracy: 0.8333\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6774 - accuracy: 0.8333\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6765 - accuracy: 0.8333\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6756 - accuracy: 0.8333\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6746 - accuracy: 0.8333\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6736 - accuracy: 0.8333\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6726 - accuracy: 0.8333\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6715 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6703 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6691 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6679 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6666 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6652 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6638 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6624 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6608 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6592 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6576 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6558 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6540 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6520 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6500 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6480 - accuracy: 0.8333\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6458 - accuracy: 0.8333\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6435 - accuracy: 0.8333\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6411 - accuracy: 0.8333\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6386 - accuracy: 0.8333\n",
            "[]\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0.41279343 0.58720654]]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "#ЗАВДАННЯ. Сентимент-Аналіз\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Задані тексти\n",
        "texts_true = [\n",
        "    \"Це текст, який містить позитивну інформацію.\",\n",
        "    \"Ще один приклад позитивного тексту.\",\n",
        "    \"Тексти можуть містити різні емоції, включаючи позитивні.\"\n",
        "]\n",
        "\n",
        "texts_false = [\n",
        "    \"Це текст, який містить негативну інформацію.\",\n",
        "    \"Не всі тексти будуть позитивними.\",\n",
        "    \"Можна зустріти і негативні тексти.\"\n",
        "]\n",
        "\n",
        "# Об'єднання текстів\n",
        "texts = texts_true + texts_false\n",
        "count_true = len(texts_true)\n",
        "count_false = len(texts_false)\n",
        "total_lines = count_true + count_false\n",
        "print(count_true, count_false, total_lines)\n",
        "\n",
        "maxWordsCount = 1000\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»', lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "dist = list(tokenizer.word_counts.items())\n",
        "print(dist[:10])\n",
        "print(texts[0][:100])\n",
        "\n",
        "max_text_len = 10\n",
        "data = tokenizer.texts_to_sequences(texts)\n",
        "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
        "print(data_pad)\n",
        "\n",
        "print(list(tokenizer.word_index.items()))\n",
        "\n",
        "X = data_pad\n",
        "Y = np.array([[1, 0]]*count_true + [[0, 1]]*count_false)\n",
        "print(X.shape, Y.shape)\n",
        "\n",
        "indices = np.random.choice(X.shape[0], size=X.shape[0], replace=False)\n",
        "X = X[indices]\n",
        "Y = Y[indices]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(maxWordsCount, 128, input_length=max_text_len))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))\n",
        "\n",
        "history = model.fit(X, Y, batch_size=32, epochs=50)\n",
        "\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "def sequence_to_text(list_of_indices):\n",
        "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "    return(words)\n",
        "\n",
        "t = \"Я люблю позитивний настрій\".lower()\n",
        "data = tokenizer.texts_to_sequences([t])\n",
        "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
        "print(sequence_to_text(data[0]))\n",
        "\n",
        "res = model.predict(data_pad)\n",
        "print(res, np.argmax(res), sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Визначення результату\n",
        "if np.argmax(res) == 0:\n",
        "    print(\"Це негативний текст.\")\n",
        "else:\n",
        "    print(\"Це позитивний  текст.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKZguTvlwXgC",
        "outputId": "6d52fef2-cdf1-4f74-a35d-1b55de26f53b"
      },
      "id": "vKZguTvlwXgC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Це позитивний  текст.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9fa7dca-089b-4a27-9b28-c758ad24340f",
      "metadata": {
        "id": "b9fa7dca-089b-4a27-9b28-c758ad24340f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}